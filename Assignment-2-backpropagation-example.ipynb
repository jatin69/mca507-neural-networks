{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-2-backpropagation-example",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jatin69/mca507-neural-networks/blob/master/Assignment-2-backpropagation-example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOsqunoNhuCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "from math import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0LBAyFAjDlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ANN(object):\n",
        "  \"\"\"\n",
        "  A artifical neural network class\n",
        "\n",
        "  Note that -\n",
        "  In an actual implementation, the number of layers, and \n",
        "  the no of neurons in each layer will be provided as input\n",
        "  and their weights will be randomly initialized\n",
        "\n",
        "  For now, the parameters are as follows -\n",
        "  Input layer with 4 neurons\n",
        "  1 hidden layer - with 3 neurons\n",
        "  Output layer with 1 neurons\n",
        "\n",
        "  All the initial weights and biases have been hardcoded for this example. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \n",
        "    # layers\n",
        "    self.inputSize = 4\n",
        "    self.hiddenSize = 3\n",
        "    self.outputSize = 1\n",
        "    \n",
        "    # weights and biases\n",
        "    \n",
        "    # weights and biases from layer 1 (input) to layer 2 (hidden)\n",
        "    # w(i,j) - states weight from neuron i of layer 1 to neuron j of layer 2\n",
        "    self.W1 = np.array([\n",
        "                        [0.7,   # weight of w11\n",
        "                         0.4],  # weight of w12\n",
        "                        [0.2,   # weight of w21 \n",
        "                         -0.4], # weight of w22\n",
        "                        [-0.5,  # weight of w31\n",
        "                         -0.5], # weight of w32\n",
        "                        [-0.9, # bias for h1\n",
        "                         -0.6]  # bias of h2\n",
        "                      ])\n",
        "    \n",
        "    # weights and biases from layer 2 (hidden) to layer 3 (output)\n",
        "    # w(i,j) - states weight from neuron i of layer 2 to neuron j of layer 3\n",
        "    self.W2 = np.array([\n",
        "                        0.7, # weight of w11\n",
        "                        0.4, # weight of w21\n",
        "                        0.8  # bias for o1\n",
        "                      ])\n",
        "    \n",
        "    # other variables\n",
        "\n",
        "    # learning rate\n",
        "    self.eta = 0.25\n",
        "\n",
        "    self.z1 = 0\n",
        "    self.z2 = 0\n",
        "    self.z3 = 0 \n",
        "  \n",
        "    self.o_error = 0\n",
        "    self.o_delta = 0\n",
        "    \n",
        "    self.z2_error = 0\n",
        "    self.z2_delta = 0\n",
        "    \n",
        "  # activation function\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    return 1/(1+np.exp(-s))\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward propagation through our network\n",
        "    \"\"\"\n",
        "    \n",
        "    # dot product of X (input) and first set of 3x2 weights\n",
        "    self.z = np.dot(X, self.W1) \n",
        "    # print(\"Z: \",self.z)\n",
        "    \n",
        "    # activation function\n",
        "    self.z2 = self.sigmoid(self.z) \n",
        "    # print(\"Sigmoid \",self.z2)\n",
        "\n",
        "    self.z2 = np.append(self.z2, [1])\n",
        "    # print(\"Sigmoid \",self.z2)\n",
        "\n",
        "    # dot product of hidden layer (z2) and second set of 3x1 weights\n",
        "    self.z3 = np.dot(self.z2, self.W2)\n",
        "    # activation function \n",
        "    output = self.sigmoid(self.z3) \n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "  def sigmoidPrime(self, s):\n",
        "    \"\"\"\n",
        "    derivative of sigmoid\n",
        "    \"\"\"\n",
        "    return s * (1 - s)\n",
        "\n",
        "\n",
        "  def backPropagate(self, X, y, o):\n",
        "    \"\"\"\n",
        "    backpropagate the error\n",
        "    \"\"\"\n",
        "\n",
        "    # step 1 : calculate error \n",
        "    \n",
        "    # error in output\n",
        "    self.o_error = y - o \n",
        "    learning_rate = self.eta\n",
        "      \n",
        "    # applying derivative of sigmoid to error\n",
        "    self.o_delta = self.o_error * self.sigmoidPrime(o) \n",
        "    self.z2_error = learning_rate * self.o_delta * self.z2 \n",
        "    # print(\"\\n\\nDelta output layer: \",self.z2_error)\n",
        "      \n",
        "    temp = [a*b for a,b in zip(self.sigmoidPrime(self.z2), self.W2 )]\n",
        "    self.z2_delta = learning_rate * self.o_delta * temp\n",
        "      \n",
        "    # step 2 : calculate adjustment to weights and biases\n",
        "    \n",
        "    # calculate adjustment for weights\n",
        "    \n",
        "    delta = []\n",
        "      \n",
        "    for i in range (self.inputSize):\n",
        "      adjustedWeights = []\n",
        "      for j in range (self.hiddenSize-1):\n",
        "        adjustedWeights.append(X[i] * self.z2_delta[j])\n",
        "      delta.append(adjustedWeights)\n",
        "      \n",
        "    # calculate adjustment for biases\n",
        "    \n",
        "    bias_update = []\n",
        "    for i in range (self.hiddenSize-1):\n",
        "      bias_update.append(self.W1[self.inputSize-1][i])\n",
        "\n",
        "    delta.append(bias_update)\n",
        "    # print(\"Delta : \", delta)\n",
        "\n",
        "    # step 3 : adjust weights and biases\n",
        "    \n",
        "    # update weights\n",
        "    \n",
        "    self.W1 = [a+b for a,b in zip(delta, self.W1)]\n",
        "    print(\"\\n\\nUpdated Weights from input to hidden layer: \")\n",
        "    for i in range(len(self.W1)):\n",
        "      for j in range(len(self.W1[0])):\n",
        "        print(\"W\" + str(i) + str(j) + ': ',self.W1[i][j])\n",
        "  \n",
        "    # update biases\n",
        "    \n",
        "    self.W2 = [a+b for a,b in zip(self.z2_error, self.W2)]\n",
        "    print(\"\\n\\nUpdated Weights from hidden to output layer: \")\n",
        "    for i in range(len(self.W2)):\n",
        "      print(\"W\" + str(i) +   ': ',self.W2[i])\n",
        "      \n",
        "  def train (self, X, y):\n",
        "    o = self.forward(X)\n",
        "    # print(\"O: \",o)\n",
        "    self.backPropagate(X, y, o)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyPmu7m_j-_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6399fcd3-75f7-4076-c24e-2bc48b6702f9"
      },
      "source": [
        "# input values\n",
        "X = np.array([1.7, 0.8, 1.3, 1])\n",
        "print(\"Initial Input: \" + str(X)) \n",
        "\n",
        "# output value\n",
        "Y = np.array([0.45])\n",
        "print(\"Expected Output: \" + str(Y)) "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Input: [1.7 0.8 1.3 1. ]\n",
            "Expected Output: [0.45]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQrQ3jH1kBAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialise a neural network\n",
        "NN = ANN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXDaVh25kG0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa8fb277-5bd0-4e18-e18e-ca0b84ae8703"
      },
      "source": [
        "# feed forward one time\n",
        "epoch_output = NN.forward(X)\n",
        "print (\"Predicted Output: \" + str(epoch_output)) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Output: 0.7740915173532013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FcG7R_NkVTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03fff87c-54dc-419e-ddef-1ccc9278de6e"
      },
      "source": [
        "# mean square error for this epoch\n",
        "epoch_mse = np.mean(np.square(Y - NN.forward(X)))\n",
        "print (\"Loss: \" + str(epoch_mse)) # mean sum squared loss\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.10503531162030035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5XhDeFxkWqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6d146de2-3e01-4db0-8c97-e23bc94e3384"
      },
      "source": [
        "# simply train this neural network to fit the output\n",
        "NN.train(X, Y)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Updated Weights from input to hidden layer: \n",
            "W00:  0.6835815680561265\n",
            "W01:  0.39220842837700715\n",
            "W10:  0.19227367908523602\n",
            "W11:  -0.40366662194023195\n",
            "W20:  -0.5125552714864915\n",
            "W21:  -0.505958260652877\n",
            "W30:  -0.909657901143455\n",
            "W31:  -0.6045832774252899\n",
            "\n",
            "\n",
            "Updated Weights from hidden to output layer: \n",
            "W0:  0.6748172801270177\n",
            "W1:  0.38363525034886925\n",
            "W2:  0.7433523820096567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4VV5U5PkX-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}